{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f4d4c0-44f8-4885-a6cd-f6e0432920cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import torch\n",
    "from torch import nn\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b257be7c-4afc-4b81-835a-b3c141937c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4co.envs import SSPEnv\n",
    "from rl4co.models.zoo.am import AttentionModelPolicy, AttentionModel\n",
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "from rl4co.utils.decoding import random_policy, rollout\n",
    "from rl4co.utils.ops import gather_by_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0e76b1-53a3-4b2e-8979-72cfa16b7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSPInitEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, fixed_len, linear_bias=True):\n",
    "        super(SSPInitEmbedding, self).__init__()\n",
    "        node_dim = fixed_len  # x, y\n",
    "        self.init_embed = nn.Linear(node_dim, embedding_dim, linear_bias)\n",
    "\n",
    "    def forward(self, td):\n",
    "        out = self.init_embed(td[\"codes\"])\n",
    "        return out\n",
    "\n",
    "class SSPContext(nn.Module):\n",
    "    \"\"\"Context embedding for the Traveling Salesman Problem (TSP).\n",
    "    Project the following to the embedding space:\n",
    "        - first node embedding\n",
    "        - current node embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim,  linear_bias=True):\n",
    "        super(SSPContext, self).__init__()\n",
    "        self.W_placeholder = nn.Parameter(\n",
    "            torch.Tensor(embedding_dim).uniform_(-1, 1)\n",
    "        )\n",
    "        self.project_context = nn.Linear(\n",
    "            embedding_dim, embedding_dim, bias=linear_bias\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings, td):\n",
    "        batch_size = embeddings.size(0)\n",
    "        # By default, node_dim = -1 (we only have one node embedding per node)\n",
    "        node_dim = (\n",
    "            (-1,) if td[\"current_node\"].dim() == 1 else (td[\"current_node\"].size(-1), -1)\n",
    "        )\n",
    "        if td[\"i\"][(0,) * td[\"i\"].dim()].item() < 1:  # get first item fast\n",
    "            context_embedding = self.W_placeholder[None, :].expand(\n",
    "                batch_size, self.W_placeholder.size(-1)\n",
    "            )\n",
    "        else:\n",
    "            context_embedding = gather_by_index(\n",
    "                embeddings,\n",
    "                torch.stack([td[\"current_node\"]], -1).view(\n",
    "                    batch_size, -1\n",
    "                ),\n",
    "            ).view(batch_size, *node_dim)\n",
    "        return self.project_context(context_embedding)\n",
    "        \n",
    "class StaticEmbedding(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(StaticEmbedding, self).__init__()\n",
    "\n",
    "    def forward(self, td):\n",
    "        return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b06951-f8dd-47d9-94c6-e5f563d159ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "val_file not set. Generating dataset instead\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96fb49afbac45bcb41dc8b2b763a93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                        | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/reward        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -9.230250358581543     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test/reward       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -9.230250358581543    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_loc = 100\n",
    "fixed_len = 15\n",
    "emb_dim = 128\n",
    "\n",
    "env = SSPEnv(generator_params={\"num_loc\":num_loc,\n",
    "                              \"fixed_len\":fixed_len},\n",
    "            test_file = \"data_ssp.npz\")\n",
    "\n",
    "checkpoint_path = \"/home/yining/ssp/rl4co/checkpoints_ssp_old/epoch_epoch=813.ckpt\"\n",
    "\n",
    "# Model: default is AM with REINFORCE and greedy rollout baseline\n",
    "model = AttentionModel.load_from_checkpoint(checkpoint_path)\n",
    "model.env = env\n",
    "model.data_cfg[\"test_batch_size\"] = 10000\n",
    "model.data_cfg[\"test_data_size\"] = 10000\n",
    "\n",
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "\n",
    "# We use our own wrapper around Lightning's `Trainer` to make it easier to use\n",
    "trainer = RL4COTrainer(max_epochs=1000, \n",
    "                       accelerator = 'gpu', \n",
    "                       devices=1,   \n",
    "                       # logger=logger,\n",
    "                       # callbacks=callbacks,\n",
    "                      )\n",
    "\n",
    "out = trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e581f2c-768d-4126-86c7-b30db041ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL constructive length:\t 923.03,\tgap: 0.00%,\t time: <1s \t(GPU in parallel)\n",
      "------------------------------------------------------------------------------------------\n",
      "15-mers-greedy length:\t 1497.43,\tgap: 62.23%,\t time: 2s \t(CPU in series)\n",
      "14-mers-greedy length:\t 1488.60,\tgap: 61.27%,\t time: 3s \t(CPU in series)\n",
      "10-mers-greedy length:\t 1322.88,\tgap: 43.32%,\t time: 34s \t(CPU in series)\n",
      "5-mers-greedy length:\t 849.76,\tgap: -7.94%,\t time: 1m59s \t(CPU in series)\n",
      "4-mers-greedy length:\t 824.67,\tgap: -10.66%,\t time: 2m54s \t(CPU in series)\n",
      "3-mers-greedy length:\t 812.39,\tgap: -11.99%,\t time: 3m58s \t(CPU in series)\n",
      "2-mers-greedy length:\t 806.99,\tgap: -12.57%,\t time: 6m47s \t(CPU in series)\n",
      "1-mers-greedy length:\t 805.01,\tgap: -12.79%,\t time: 11m57s \t(CPU in series)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ks = [1, 2, 3, 4, 5, 10, 14, 15][::-1]\n",
    "time = ['11m57s', '6m47s',  '3m58s','2m54s','1m59s', '34s', '3s', '2s'][::-1]\n",
    "RL = out[0][\"test/reward\"] * env.generator.num_loc * -1\n",
    "print(f\"RL constructive length:\\t {RL:.2f},\\tgap: {0:.2f}%,\\t time: <1s \\t(GPU in parallel)\")\n",
    "print('-' * 90)\n",
    "\n",
    "for k, t in zip(ks, time):\n",
    "    greedy_baseline = np.load(f'greedy_{k}-mers_output.npz')[\"arr_0\"].mean()\n",
    "    gap = (greedy_baseline - RL) / RL * 100\n",
    "    print(f\"{k}-mers-greedy length:\\t {greedy_baseline:.2f},\\tgap: {gap:.2f}%,\\t time: {t} \\t(CPU in series)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4603bc1e-afff-4d73-8ffb-ac1ab5b2092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.tensordict import TensorDict\n",
    "from tqdm import tqdm\n",
    "td = TensorDict({\"codes\": torch.tensor(np.load(\"data/data_ssp.npz\")[\"codes\"])}, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3514fb43-de23-44f4-b35a-4a6ec401d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▎                                                            | 100/400 [00:38<01:56,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: 100 length: 899.8259544372559 gap: -2.513375100311315 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████▌                                        | 200/400 [01:18<01:23,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: 200 length: 897.3359107971191 gap: -2.7831449920696327 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████▊                    | 300/400 [02:00<00:42,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: 300 length: 896.1589813232422 gap: -2.910652852436902 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 400/400 [02:43<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: 400 length: 895.3390121459961 gap: -2.9994878401557084 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sampling rollouts over trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "td_init = model.env.reset(td).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "rewards_best = None\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(400)):\n",
    "        out = model(td_init.clone(), phase=\"test\", decode_type=\"sampling\", return_actions=False)\n",
    "        rewards_now = out['reward'].cpu().detach()\n",
    "        if rewards_best is None:\n",
    "            rewards_best = rewards_now\n",
    "        else:\n",
    "            rewards_best = torch.max(rewards_now, rewards_best)\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            obj = -rewards_best.mean().numpy() * env.generator.num_loc\n",
    "            print('Sampling:', i+1, 'length:', obj,  'gap:', (obj - RL) / RL * 100, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
